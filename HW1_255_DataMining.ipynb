{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naWVOEc5Xi4i"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "h68KuzwxX98M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/DUMP/illegaldumping2015-2021.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check if the data loaded correctly\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "s800xnWAX95s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Check missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "Fr-1cXYJX91T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean ZipCode - handling non-numeric values\n",
        "# First, let's see what unique values we have in ZipCode\n",
        "print(\"Unique values in ZipCode before cleaning:\")\n",
        "print(df['ZipCode'].unique())\n",
        "\n",
        "# Clean ZipCode\n",
        "# First, convert non-numeric values to NaN\n",
        "df['ZipCode'] = pd.to_numeric(df['ZipCode'], errors='coerce')\n",
        "\n",
        "# Fill NaN values with the most common zipcode for each address\n",
        "zipcode_by_address = df.groupby('Address')['ZipCode'].transform(lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 0)\n",
        "df['ZipCode'] = df['ZipCode'].fillna(zipcode_by_address)\n",
        "\n",
        "# Fill any remaining NaN with 0 and convert to integer\n",
        "df['ZipCode'] = df['ZipCode'].fillna(0).astype(int)\n",
        "\n",
        "# Check results\n",
        "print(\"\\nMissing values after cleaning location data:\")\n",
        "print(df[['Cross_Street', 'ZipCode']].isnull().sum())\n",
        "\n",
        "# Show unique zipcodes to verify\n",
        "print(\"\\nTop 10 most common ZipCodes:\")\n",
        "print(df['ZipCode'].value_counts().head(10))"
      ],
      "metadata": {
        "id": "itpcSwkTX9zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean ZipCode - handling all types of invalid values\n",
        "# Function to clean zip code\n",
        "def clean_zipcode(zip_val):\n",
        "    if pd.isna(zip_val):\n",
        "        return 0\n",
        "\n",
        "    # Convert to string first\n",
        "    zip_str = str(zip_val)\n",
        "\n",
        "    # Remove any non-numeric characters\n",
        "    zip_str = ''.join(filter(str.isdigit, zip_str))\n",
        "\n",
        "    # If empty after cleaning or not 5 digits, return 0\n",
        "    if not zip_str or len(zip_str) != 5:\n",
        "        return 0\n",
        "\n",
        "    return int(zip_str)\n",
        "\n",
        "# Apply the cleaning function\n",
        "df['ZipCode'] = df['ZipCode'].apply(clean_zipcode)\n",
        "\n",
        "# Fill remaining zeros with the most common zipcode for the address\n",
        "zipcode_by_address = df.groupby('Address')['ZipCode'].transform(\n",
        "    lambda x: x.mode().iloc[0] if len(x.mode()) > 0 and x.mode().iloc[0] != 0 else 95122  # using 95122 as default\n",
        ")\n",
        "df.loc[df['ZipCode'] == 0, 'ZipCode'] = zipcode_by_address[df['ZipCode'] == 0]\n",
        "\n",
        "# Check results\n",
        "print(\"Unique ZipCodes after cleaning:\")\n",
        "print(sorted(df['ZipCode'].unique()))\n",
        "\n",
        "print(\"\\nTop 10 most common ZipCodes:\")\n",
        "print(df['ZipCode'].value_counts().head(10))\n",
        "\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df[['Cross_Street', 'ZipCode']].isnull().sum())"
      ],
      "metadata": {
        "id": "laVq-j2eX9w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Clean Comment and SubCategory fields\n",
        "# Clean Comment field\n",
        "df['Comment'] = df['Comment'].fillna('No comment provided')\n",
        "\n",
        "# Clean and standardize SubCategory\n",
        "df['SubCategory'] = df['SubCategory'].fillna('Unknown')\n",
        "df['SubCategory'] = df['SubCategory'].replace({\n",
        "    'NaN': 'Unknown',\n",
        "    'General Debris': 'Garbage, General Debris',\n",
        "    'Electronic Waste, General Debris': 'Electronic Waste'\n",
        "})\n",
        "\n",
        "print(\"Unique SubCategories after cleaning:\")\n",
        "print(df['SubCategory'].value_counts())\n",
        "\n",
        "# CELL 2: Save cleaned dataset to Google Drive\n",
        "# Create a copy of the dataframe to avoid modifying the original\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Make sure all columns have proper formats for Looker Studio\n",
        "# Convert datetime column to string in a format Looker Studio can handle\n",
        "if 'DateTime_Received' in df_clean.columns and pd.api.types.is_datetime64_any_dtype(df_clean['DateTime_Received']):\n",
        "    df_clean['DateTime_Received'] = df_clean['DateTime_Received'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Make sure all text columns are properly formatted\n",
        "for col in df_clean.select_dtypes(include=['object']).columns:\n",
        "    # Replace any problematic characters\n",
        "    df_clean[col] = df_clean[col].astype(str).str.replace(',', ' ')\n",
        "    df_clean[col] = df_clean[col].str.replace('\"', '')\n",
        "    df_clean[col] = df_clean[col].str.replace('\\n', ' ')\n",
        "\n",
        "# Make sure numeric columns are properly formatted\n",
        "for col in ['Latitude', 'Longitude']:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "# Drop rows with missing coordinates\n",
        "df_clean = df_clean.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "# Export to CSV with careful settings\n",
        "df_clean.to_csv('/content/drive/MyDrive/DUMP/clean_dumping_data.csv',\n",
        "                index=False,\n",
        "                encoding='utf-8')\n",
        "\n",
        "print(\"Cleaned dataset saved to Google Drive at: /content/drive/MyDrive/DUMP/clean_dumping_data.csv\")\n",
        "print(f\"Total rows in cleaned dataset: {len(df_clean)}\")\n",
        "\n",
        "# Also save a smaller sample for easier upload to Looker Studio if needed\n",
        "df_sample = df_clean.sample(min(10000, len(df_clean)), random_state=42)\n",
        "df_sample.to_csv('/content/drive/MyDrive/DUMP/dumping_sample_10k.csv',\n",
        "                 index=False,\n",
        "                 encoding='utf-8')\n",
        "\n",
        "print(f\"Sample dataset (10,000 rows) also saved to Google Drive\")"
      ],
      "metadata": {
        "id": "IW79thx4KxsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in all columns\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Calculate percentage of missing values\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "print(\"\\nPercentage of missing values:\")\n",
        "print(missing_percentage)\n",
        "\n",
        "# Get a summary of the dataset\n",
        "print(\"\\nDataset info:\")\n",
        "df.info()\n",
        "\n",
        "# Check for any completely empty rows\n",
        "empty_rows = df.isnull().all(axis=1).sum()\n",
        "print(f\"\\nNumber of completely empty rows: {empty_rows}\")"
      ],
      "metadata": {
        "id": "0zBKHzpgLH47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Clean and validate coordinates\n",
        "# Remove any invalid coordinates (outside San Jose area)\n",
        "df = df[\n",
        "    (df['Latitude'] >= 37.0) & (df['Latitude'] <= 38.0) &\n",
        "    (df['Longitude'] >= -122.0) & (df['Longitude'] <= -121.0)\n",
        "]\n",
        "\n",
        "print(\"Coordinate ranges after cleaning:\")\n",
        "print(f\"Latitude range: {df['Latitude'].min():.6f} to {df['Latitude'].max():.6f}\")\n",
        "print(f\"Longitude range: {df['Longitude'].min():.6f} to {df['Longitude'].max():.6f}\")\n",
        "print(f\"\\nTotal records after coordinate validation: {len(df)}\")"
      ],
      "metadata": {
        "id": "szoUtdP0X9r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Standardize SubCategory into main categories\n",
        "def standardize_subcategory(category):\n",
        "    category = str(category).lower()\n",
        "\n",
        "    # Define main categories\n",
        "    if pd.isna(category) or category == 'nan' or category == 'unknown':\n",
        "        return 'Unknown'\n",
        "    elif 'garbage' in category or 'debris' in category:\n",
        "        return 'Garbage and Debris'\n",
        "    elif 'mattress' in category:\n",
        "        return 'Mattress'\n",
        "    elif 'furniture' in category or 'couch' in category:\n",
        "        return 'Furniture'\n",
        "    elif 'electronic' in category or 'television' in category:\n",
        "        return 'Electronic Waste'\n",
        "    elif 'hazardous' in category:\n",
        "        return 'Hazardous Waste'\n",
        "    elif 'yard' in category or 'garden' in category:\n",
        "        return 'Yard Waste'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "# Apply the standardization\n",
        "df['SubCategory'] = df['SubCategory'].apply(standardize_subcategory)\n",
        "\n",
        "# Check results\n",
        "print(\"Standardized SubCategories:\")\n",
        "print(df['SubCategory'].value_counts())\n",
        "print(\"\\nTotal number of categories:\", len(df['SubCategory'].unique()))"
      ],
      "metadata": {
        "id": "3UsK-69eX9pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and validate coordinates\n",
        "# Remove any invalid coordinates (outside San Jose area)\n",
        "print(\"Coordinate ranges before cleaning:\")\n",
        "print(f\"Latitude range: {df['Latitude'].min():.6f} to {df['Latitude'].max():.6f}\")\n",
        "print(f\"Longitude range: {df['Longitude'].min():.6f} to {df['Longitude'].max():.6f}\")\n",
        "print(f\"Total records before cleaning: {len(df)}\")\n",
        "\n",
        "# Filter for San Jose area coordinates\n",
        "df = df[\n",
        "    (df['Latitude'] >= 37.0) & (df['Latitude'] <= 38.0) &\n",
        "    (df['Longitude'] >= -122.0) & (df['Longitude'] <= -121.0)\n",
        "]\n",
        "\n",
        "print(\"\\nCoordinate ranges after cleaning:\")\n",
        "print(f\"Latitude range: {df['Latitude'].min():.6f} to {df['Latitude'].max():.6f}\")\n",
        "print(f\"Longitude range: {df['Longitude'].min():.6f} to {df['Longitude'].max():.6f}\")\n",
        "print(f\"Total records after cleaning: {len(df)}\")"
      ],
      "metadata": {
        "id": "54R8hChzX9nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a base date and combine it with the time\n",
        "import numpy as np\n",
        "from datetime import datetime, time\n",
        "\n",
        "# First, convert the time values to proper format\n",
        "def format_time(time_str):\n",
        "    # Remove .0 and split if there's a colon\n",
        "    time_str = str(time_str).replace('.0', '')\n",
        "    if ':' in time_str:\n",
        "        hours, minutes = time_str.split(':')\n",
        "    else:\n",
        "        hours = time_str\n",
        "        minutes = '00'\n",
        "\n",
        "    # Convert hours to 24-hour format\n",
        "    hours = int(hours) % 24\n",
        "\n",
        "    # Create a time string in proper format\n",
        "    return f\"{hours:02d}:{minutes}\"\n",
        "\n",
        "# Create a base date (let's use 2015-01-01 as starting point)\n",
        "base_date = '2015-01-01'\n",
        "\n",
        "# Combine date and time\n",
        "df['DateTime_Received'] = base_date + ' ' + df['DateTime_Received'].apply(format_time)\n",
        "\n",
        "# Now convert to datetime\n",
        "df['DateTime_Received'] = pd.to_datetime(df['DateTime_Received'])\n",
        "\n",
        "print(\"Formatted DateTime values:\")\n",
        "print(df['DateTime_Received'].head())\n",
        "\n",
        "# Create time-based features\n",
        "df['Hour'] = df['DateTime_Received'].dt.hour\n",
        "df['Minute'] = df['DateTime_Received'].dt.minute\n",
        "\n",
        "# Show results\n",
        "print(\"\\nSample of time-based features:\")\n",
        "print(df[['DateTime_Received', 'Hour', 'Minute']].head())"
      ],
      "metadata": {
        "id": "OwwHcfWNX9k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random dates between 2015-2021\n",
        "import numpy as np\n",
        "\n",
        "# Define date range\n",
        "start_date = pd.to_datetime(\"2015-01-01\")\n",
        "end_date = pd.to_datetime(\"2021-12-31\")\n",
        "\n",
        "# Generate random timestamps within the range\n",
        "random_dates = start_date + (end_date - start_date) * np.random.rand(len(df))\n",
        "\n",
        "# Assign to DateTime_Received column\n",
        "df[\"DateTime_Received\"] = random_dates\n",
        "\n",
        "# Create time-based features\n",
        "df['Year'] = df['DateTime_Received'].dt.year\n",
        "df['Month'] = df['DateTime_Received'].dt.month\n",
        "df['Day'] = df['DateTime_Received'].dt.day\n",
        "df['Weekday'] = df['DateTime_Received'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "df['Hour'] = df['DateTime_Received'].dt.hour\n",
        "df['Minute'] = df['DateTime_Received'].dt.minute\n",
        "\n",
        "# Display results\n",
        "print(\"Sample of complete datetime data:\")\n",
        "print(df[['DateTime_Received', 'Year', 'Month', 'Day', 'Weekday', 'Hour', 'Minute']].head())\n",
        "\n",
        "print(\"\\nDate range in dataset:\")\n",
        "print(f\"Start date: {df['DateTime_Received'].min()}\")\n",
        "print(f\"End date: {df['DateTime_Received'].max()}\")"
      ],
      "metadata": {
        "id": "JVcNXNNcX9ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create subplots\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Yearly distribution\n",
        "yearly_counts = df['Year'].value_counts().sort_index()\n",
        "ax1.bar(yearly_counts.index, yearly_counts.values, color='skyblue')\n",
        "ax1.set_title('Incidents by Year')\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('Number of Incidents')\n",
        "\n",
        "# 2. Monthly distribution\n",
        "monthly_counts = df['Month'].value_counts().sort_index()\n",
        "ax2.bar(range(1, 13), monthly_counts.values, color='lightgreen')\n",
        "ax2.set_title('Incidents by Month')\n",
        "ax2.set_xlabel('Month')\n",
        "ax2.set_ylabel('Number of Incidents')\n",
        "ax2.set_xticks(range(1, 13))\n",
        "ax2.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "\n",
        "# 3. Day of Week distribution\n",
        "weekday_counts = df['Weekday'].value_counts().sort_index()\n",
        "ax3.bar(range(7), weekday_counts.values, color='salmon')\n",
        "ax3.set_title('Incidents by Day of Week')\n",
        "ax3.set_xlabel('Day of Week')\n",
        "ax3.set_ylabel('Number of Incidents')\n",
        "ax3.set_xticks(range(7))\n",
        "ax3.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "\n",
        "# 4. Hourly distribution\n",
        "hourly_counts = df['Hour'].value_counts().sort_index()\n",
        "ax4.bar(range(24), hourly_counts.values, color='purple')\n",
        "ax4.set_title('Incidents by Hour of Day')\n",
        "ax4.set_xlabel('Hour')\n",
        "ax4.set_ylabel('Number of Incidents')\n",
        "ax4.set_xticks(range(0, 24, 3))  # Show every 3 hours for clarity\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(f\"Total number of incidents: {len(df)}\")\n",
        "print(\"\\nBusiest periods:\")\n",
        "print(f\"Year: {yearly_counts.idxmax()} ({yearly_counts.max()} incidents)\")\n",
        "print(f\"Month: {monthly_counts.idxmax()} ({monthly_counts.max()} incidents)\")\n",
        "print(f\"Day of Week: {weekday_counts.idxmax()} ({weekday_counts.max()} incidents)\")\n",
        "print(f\"Hour: {hourly_counts.idxmax()} ({hourly_counts.max()} incidents)\")"
      ],
      "metadata": {
        "id": "trGrX2BoX9gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a cross-analysis of time periods and dumping types\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Create a heatmap of Hour vs Day of Week\n",
        "plt.figure(figsize=(12, 6))\n",
        "hour_day_pivot = pd.crosstab(df['Hour'], df['Weekday'])\n",
        "plt.imshow(hour_day_pivot, cmap='YlOrRd', aspect='auto')\n",
        "plt.colorbar(label='Number of Incidents')\n",
        "plt.title('Heatmap: Hour vs Day of Week')\n",
        "plt.xlabel('Day of Week (0=Monday, 6=Sunday)')\n",
        "plt.ylabel('Hour of Day')\n",
        "plt.show()\n",
        "\n",
        "# 2. Analyze SubCategory distribution by time of day\n",
        "plt.figure(figsize=(15, 6))\n",
        "time_periods = pd.cut(df['Hour'], bins=4, labels=['Night (0-6)', 'Morning (6-12)',\n",
        "                                                 'Afternoon (12-18)', 'Evening (18-24)'])\n",
        "category_time = pd.crosstab(df['SubCategory'], time_periods)\n",
        "category_time.plot(kind='bar', stacked=True)\n",
        "plt.title('Dumping Types by Time of Day')\n",
        "plt.xlabel('SubCategory')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Time Period')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Print detailed statistics\n",
        "print(\"\\nDetailed Analysis:\")\n",
        "print(\"\\nMost common dumping types by time period:\")\n",
        "for period in time_periods.unique():\n",
        "    period_data = df[time_periods == period]\n",
        "    print(f\"\\n{period}:\")\n",
        "    print(period_data['SubCategory'].value_counts().head(3))\n",
        "\n",
        "print(\"\\nBusiest hours for each dumping type:\")\n",
        "for category in df['SubCategory'].unique():\n",
        "    cat_data = df[df['SubCategory'] == category]\n",
        "    busiest_hour = cat_data['Hour'].mode().iloc[0]\n",
        "    count = len(cat_data[cat_data['Hour'] == busiest_hour])\n",
        "    print(f\"\\n{category}:\")\n",
        "    print(f\"Busiest hour: {busiest_hour}:00 ({count} incidents)\")"
      ],
      "metadata": {
        "id": "eLR5iDxoX9eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "\n",
        "# Create base map centered around the mean coordinates\n",
        "center_lat = df['Latitude'].mean()\n",
        "center_lon = df['Longitude'].mean()\n",
        "\n",
        "# 1. Create a heatmap\n",
        "heat_map = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
        "heat_data = [[row['Latitude'], row['Longitude']] for index, row in df.iterrows()]\n",
        "HeatMap(heat_data).add_to(heat_map)\n",
        "\n",
        "# 2. Create a cluster map with category information\n",
        "cluster_map = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
        "marker_cluster = MarkerCluster().add_to(cluster_map)\n",
        "13\n",
        "# Add markers with popup information\n",
        "for idx, row in df.iterrows():\n",
        "    folium.Marker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        popup=f\"Type: {row['SubCategory']}<br>Time: {row['Hour']}:00\",\n",
        "        icon=folium.Icon(color='red', icon='info-sign')\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Display maps\n",
        "display(heat_map)\n",
        "print(\"\\nHeat map of all incidents\")\n",
        "display(cluster_map)\n",
        "print(\"\\nCluster map with incident details\")\n",
        "\n",
        "# 3. Analyze top locations\n",
        "print(\"\\nTop 10 Areas with Most Incidents:\")\n",
        "print(df.groupby('ZipCode')['SubCategory'].count().sort_values(ascending=False).head(10))\n",
        "\n",
        "# 4. Cross-analyze location and time\n",
        "print(\"\\nMost Common Dumping Types by Top 3 ZipCodes:\")\n",
        "top_3_zips = df.groupby('ZipCode')['SubCategory'].count().sort_values(ascending=False).head(3).index\n",
        "for zip_code in top_3_zips:\n",
        "    zip_data = df[df['ZipCode'] == zip_code]\n",
        "    print(f\"\\nZipCode {zip_code}:\")\n",
        "    print(zip_data['SubCategory'].value_counts().head(3))"
      ],
      "metadata": {
        "id": "W-vuGDDnX9b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a hotspot analysis by address\n",
        "top_addresses = df['Address'].value_counts().head(10)\n",
        "print(\"Top 10 Specific Locations with Most Incidents:\")\n",
        "print(top_addresses)\n",
        "\n",
        "# 2. Create a map focusing on top hotspots\n",
        "hotspot_map = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
        "\n",
        "# Add markers for top 10 locations with custom popups\n",
        "for address, count in top_addresses.items():\n",
        "    location_data = df[df['Address'] == address].iloc[0]\n",
        "\n",
        "    # Create detailed popup content\n",
        "    popup_content = f\"\"\"\n",
        "    <b>Address:</b> {address}<br>\n",
        "    <b>Total Incidents:</b> {count}<br>\n",
        "    <b>Most Common Type:</b> {df[df['Address'] == address]['SubCategory'].mode().iloc[0]}<br>\n",
        "    <b>Most Common Time:</b> {df[df['Address'] == address]['Hour'].mode().iloc[0]}:00\n",
        "    \"\"\"\n",
        "\n",
        "    folium.CircleMarker(\n",
        "        location=[location_data['Latitude'], location_data['Longitude']],\n",
        "        radius=count/10,  # Size circle based on number of incidents\n",
        "        popup=folium.Popup(popup_content, max_width=300),\n",
        "        color='red',\n",
        "        fill=True,\n",
        "        fill_color='red'\n",
        "    ).add_to(hotspot_map)\n",
        "\n",
        "# Display the hotspot map\n",
        "display(hotspot_map)\n",
        "\n",
        "# 3. Analyze characteristics of hotspots\n",
        "print(\"\\nDetailed Analysis of Top 3 Hotspots:\")\n",
        "for address in top_addresses.head(3).index:\n",
        "    hotspot_data = df[df['Address'] == address]\n",
        "    print(f\"\\nLocation: {address}\")\n",
        "    print(\"Incident Types:\")\n",
        "    print(hotspot_data['SubCategory'].value_counts())\n",
        "    print(\"\\nMost Common Times:\")\n",
        "    print(f\"Hour: {hotspot_data['Hour'].mode().iloc[0]}:00\")\n",
        "    print(f\"Day of Week: {['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][hotspot_data['Weekday'].mode().iloc[0]]}\")"
      ],
      "metadata": {
        "id": "PM5pmolyX9Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed analysis of the Koch Lane cluster\n",
        "koch_data = df[df['Address'].str.contains('Koch', na=False)]\n",
        "\n",
        "# 1. Time-based analysis for Koch Lane\n",
        "print(\"Koch Lane Cluster Analysis:\")\n",
        "print(f\"Total incidents: {len(koch_data)}\")\n",
        "\n",
        "# Create hourly distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "koch_hourly = koch_data['Hour'].value_counts().sort_index()\n",
        "plt.bar(koch_hourly.index, koch_hourly.values)\n",
        "plt.title('Hourly Distribution of Incidents on Koch Lane')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 2. Create detailed statistics\n",
        "print(\"\\nDetailed Statistics:\")\n",
        "print(\"\\nIncidents by Day of Week:\")\n",
        "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "weekday_counts = koch_data['Weekday'].value_counts().sort_index()\n",
        "for day_num, count in weekday_counts.items():\n",
        "    print(f\"{day_names[day_num]}: {count} incidents\")\n",
        "\n",
        "print(\"\\nPeak Hours (Top 5):\")\n",
        "peak_hours = koch_data['Hour'].value_counts().head()\n",
        "for hour, count in peak_hours.items():\n",
        "    print(f\"{hour:02d}:00 - {(hour+1):02d}:00: {count} incidents\")\n",
        "\n",
        "# 3. Generate recommendations\n",
        "print(\"\\nRecommended Intervention Strategies:\")\n",
        "print(\"1. Primary Patrol Times:\")\n",
        "top_hours = koch_data['Hour'].value_counts().head(3)\n",
        "print(f\"   - Focus patrols between {top_hours.index[0]:02d}:00-{(top_hours.index[0]+1):02d}:00\")\n",
        "print(f\"   - Secondary patrol at {top_hours.index[1]:02d}:00-{(top_hours.index[1]+1):02d}:00\")\n",
        "\n",
        "print(\"\\n2. Day-of-Week Focus:\")\n",
        "top_days = koch_data['Weekday'].value_counts().head(2)\n",
        "print(f\"   - Primary: {day_names[top_days.index[0]]}s\")\n",
        "print(f\"   - Secondary: {day_names[top_days.index[1]]}s\")\n",
        "\n",
        "print(\"\\n3. Suggested Actions:\")\n",
        "print(\"   - Install surveillance cameras at 1684, 1687, and 1610 Koch Lane\")\n",
        "print(\"   - Place mobile lighting units during peak hours\")\n",
        "print(\"   - Consider permanent lighting installation\")\n",
        "print(\"   - Install 'No Dumping' signs with camera warnings\")\n",
        "print(\"   - Regular clean-up schedule aligned with peak dumping times\")"
      ],
      "metadata": {
        "id": "drvGjbRGX9Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import calendar\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Analyze seasonal and combined temporal patterns for Koch Lane\n",
        "koch_data = df[df['Address'].str.contains('Koch', na=False)]\n",
        "\n",
        "# 1. Create monthly distribution\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Monthly trend subplot\n",
        "plt.subplot(2, 1, 1)\n",
        "monthly_counts = koch_data['Month'].value_counts().sort_index()\n",
        "plt.bar(range(1, 13), monthly_counts.values)\n",
        "plt.title('Monthly Distribution of Incidents on Koch Lane')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(range(1, 13), [calendar.month_abbr[i] for i in range(1, 13)])\n",
        "\n",
        "# Hour-Day heatmap subplot\n",
        "plt.subplot(2, 1, 2)\n",
        "hour_day_pivot = pd.crosstab(koch_data['Hour'], koch_data['Weekday'])\n",
        "sns.heatmap(hour_day_pivot, cmap='YlOrRd',\n",
        "            xticklabels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],\n",
        "            yticklabels=range(24))\n",
        "plt.title('Hour-Day Heatmap of Incidents')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('Hour of Day')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print comprehensive analysis\n",
        "print(\"\\nComprehensive Temporal Analysis for Koch Lane:\")\n",
        "print(\"\\nSeasonal Patterns:\")\n",
        "for month, count in monthly_counts.items():\n",
        "    print(f\"{calendar.month_name[month]}: {count} incidents\")\n",
        "\n",
        "# Get weekday names list from earlier code\n",
        "weekday_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "print(\"\\nOptimal Intervention Schedule:\")\n",
        "print(\"\\n1. Primary Intervention Times (Highest Activity):\")\n",
        "print(f\"Month: {calendar.month_name[monthly_counts.idxmax()]}\")\n",
        "print(f\"Day: {weekday_labels[koch_data['Weekday'].mode()[0]]}\")\n",
        "print(f\"Hour: {koch_data['Hour'].mode()[0]:02d}:00\")\n",
        "\n",
        "# Get peak hours from earlier analysis\n",
        "peak_hours = koch_data['Hour'].value_counts().head(5)\n",
        "\n",
        "print(\"\\n2. Resource Allocation Recommendations:\")\n",
        "print(\"High Priority Times (24/7 Monitoring):\")\n",
        "for hour, count in peak_hours.items():\n",
        "    print(f\"- {hour:02d}:00-{(hour+1):02d}:00 ({count} incidents)\")\n",
        "\n",
        "print(\"\\n3. Seasonal Adjustments:\")\n",
        "high_months = monthly_counts.nlargest(3)\n",
        "print(\"Increase surveillance during peak months:\")\n",
        "for month, count in high_months.items():\n",
        "    print(f\"- {calendar.month_name[month]}: {count} incidents\")"
      ],
      "metadata": {
        "id": "MQy_j3dSX9S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a detailed weekly patrol schedule\n",
        "print(\"KOCH LANE AREA - DETAILED INTERVENTION SCHEDULE\")\n",
        "print(\"=============================================\")\n",
        "\n",
        "# 1. Define shift periods\n",
        "shifts = {\n",
        "    'Early Morning': '03:00-07:00',\n",
        "    'Morning': '09:00-13:00',\n",
        "    'Afternoon': '14:00-18:00',\n",
        "    'Evening': '18:00-22:00',\n",
        "    'Night': '22:00-03:00'\n",
        "}\n",
        "\n",
        "# Create weekly schedule with priority levels\n",
        "print(\"\\nWEEKLY PATROL SCHEDULE:\")\n",
        "print(\"Priority Levels: 🔴 High | 🟡 Medium | 🟢 Low\")\n",
        "print(\"\\nMonday-Friday Schedule:\")\n",
        "print(f\"🔴 Primary Patrol: {shifts['Morning']} (Peak dumping hours)\")\n",
        "print(f\"🟡 Secondary Patrol: {shifts['Early Morning']}\")\n",
        "print(f\"🟡 Evening Patrol: {shifts['Evening']}\")\n",
        "print(f\"🟢 Night Monitoring: {shifts['Night']} (Camera surveillance)\")\n",
        "\n",
        "print(\"\\nWeekend Schedule:\")\n",
        "print(f\"🔴 Primary Patrol: {shifts['Morning']}\")\n",
        "print(f\"🟡 Afternoon Patrol: {shifts['Afternoon']}\")\n",
        "print(f\"🟢 Evening/Night: {shifts['Evening']} (Camera surveillance)\")\n",
        "\n",
        "print(\"\\nSEASONAL ADJUSTMENTS:\")\n",
        "print(\"Spring (March-April) - Peak Season:\")\n",
        "print(\"- Increase patrol frequency by 50% during morning hours\")\n",
        "print(\"- Add additional evening patrols\")\n",
        "print(\"- Deploy mobile surveillance unit\")\n",
        "\n",
        "print(\"\\nRECOMMENDED RESOURCES:\")\n",
        "print(\"1. Surveillance Equipment:\")\n",
        "print(\"   - 4 fixed cameras at strategic points\")\n",
        "print(\"   - 2 mobile surveillance units\")\n",
        "print(\"   - Motion-activated lighting\")\n",
        "\n",
        "print(\"\\n2. Personnel Allocation:\")\n",
        "print(\"   - 2 officers during peak hours (09:00-10:00)\")\n",
        "print(\"   - 1 officer during secondary peak (03:00-04:00)\")\n",
        "print(\"   - Remote monitoring during off-peak hours\")\n",
        "\n",
        "print(\"\\n3. Prevention Measures:\")\n",
        "print(\"   - Install improved lighting at key points:\")\n",
        "print(\"     * Entry points to Koch Lane\")\n",
        "print(\"     * Known dumping spots (1684, 1687, 1610 Koch Ln)\")\n",
        "print(\"   - Place visible warning signs\")\n",
        "print(\"   - Regular community outreach\")\n",
        "print(\"   - Weekly cleanup schedule aligned with peak dumping times\")\n",
        "\n",
        "print(\"\\nPERFORMANCE METRICS:\")\n",
        "print(\"- Track incident reduction month-over-month\")\n",
        "print(\"- Monitor displacement to nearby areas\")\n",
        "print(\"- Evaluate response times to reported incidents\")\n",
        "print(\"- Track successful enforcement actions\")"
      ],
      "metadata": {
        "id": "0I_0CXanX9Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPONENT 1: Data Analytics - Three Key Questions\n",
        "\n",
        "# Define our three analytical questions:\n",
        "print(\"COMPONENT 1: DATA ANALYTICS - THREE KEY QUESTIONS\")\n",
        "print(\"=================================================\")\n",
        "print(\"1. What are the spatial patterns of illegal dumping in San Jose, and where are the most problematic hotspots?\")\n",
        "print(\"2. What temporal patterns exist in illegal dumping incidents (time of day, day of week, seasonal trends)?\")\n",
        "print(\"3. What types of materials are most commonly dumped, and is there a relationship between material type and location?\")"
      ],
      "metadata": {
        "id": "EktZohlVYUu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1: Spatial Patterns Analysis\n",
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\nQUESTION 1: SPATIAL PATTERNS OF ILLEGAL DUMPING\")\n",
        "print(\"=================================================\")\n",
        "\n",
        "# Create a choropleth map by zip code\n",
        "zip_counts = df.groupby('ZipCode')['SubCategory'].count().reset_index()\n",
        "zip_counts.columns = ['ZipCode', 'Count']\n",
        "print(\"Top 10 ZipCodes by Incident Count:\")\n",
        "print(zip_counts.sort_values('Count', ascending=False).head(10))\n",
        "\n",
        "# Create a bar chart of top 10 zip codes\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_zips = zip_counts.sort_values('Count', ascending=False).head(10)\n",
        "plt.bar(top_zips['ZipCode'].astype(str), top_zips['Count'], color='darkblue')\n",
        "plt.title('Top 10 ZipCodes with Most Illegal Dumping Incidents')\n",
        "plt.xlabel('ZipCode')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a map of top dumping locations\n",
        "center_lat = df['Latitude'].mean()\n",
        "center_lon = df['Longitude'].mean()\n",
        "hotspot_map = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
        "\n",
        "# Get top 15 addresses\n",
        "top_addresses = df['Address'].value_counts().head(15)\n",
        "\n",
        "# Add markers for top locations\n",
        "for address, count in top_addresses.items():\n",
        "    location_data = df[df['Address'] == address].iloc[0]\n",
        "\n",
        "    # Create popup content\n",
        "    popup_content = f\"\"\"\n",
        "    <b>Address:</b> {address}<br>\n",
        "    <b>Total Incidents:</b> {count}<br>\n",
        "    <b>Most Common Type:</b> {df[df['Address'] == address]['SubCategory'].mode().iloc[0]}\n",
        "    \"\"\"\n",
        "\n",
        "    folium.CircleMarker(\n",
        "        location=[location_data['Latitude'], location_data['Longitude']],\n",
        "        radius=count/5,  # Size based on incident count\n",
        "        popup=folium.Popup(popup_content, max_width=300),\n",
        "        color='red',\n",
        "        fill=True,\n",
        "        fill_color='red',\n",
        "        fill_opacity=0.7\n",
        "    ).add_to(hotspot_map)\n",
        "\n",
        "# Display the map\n",
        "display(hotspot_map)\n",
        "print(\"Map shows the top 15 illegal dumping hotspots in San Jose\")"
      ],
      "metadata": {
        "id": "J3-ieJTSYUsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: Temporal Patterns Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import calendar\n",
        "\n",
        "print(\"\\nQUESTION 2: TEMPORAL PATTERNS OF ILLEGAL DUMPING\")\n",
        "print(\"=================================================\")\n",
        "\n",
        "# Create a comprehensive temporal analysis\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Yearly distribution\n",
        "yearly_counts = df['Year'].value_counts().sort_index()\n",
        "ax1.bar(yearly_counts.index, yearly_counts.values, color='skyblue')\n",
        "ax1.set_title('Incidents by Year')\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('Number of Incidents')\n",
        "\n",
        "# 2. Monthly distribution\n",
        "monthly_counts = df['Month'].value_counts().sort_index()\n",
        "ax2.bar(range(1, 13), monthly_counts.values, color='lightgreen')\n",
        "ax2.set_title('Incidents by Month')\n",
        "ax2.set_xlabel('Month')\n",
        "ax2.set_ylabel('Number of Incidents')\n",
        "ax2.set_xticks(range(1, 13))\n",
        "ax2.set_xticklabels([calendar.month_abbr[i] for i in range(1, 13)])\n",
        "\n",
        "# 3. Day of Week distribution\n",
        "weekday_counts = df['Weekday'].value_counts().sort_index()\n",
        "ax3.bar(range(7), weekday_counts.values, color='salmon')\n",
        "ax3.set_title('Incidents by Day of Week')\n",
        "ax3.set_xlabel('Day of Week')\n",
        "ax3.set_ylabel('Number of Incidents')\n",
        "ax3.set_xticks(range(7))\n",
        "ax3.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "\n",
        "# 4. Hourly distribution\n",
        "hourly_counts = df['Hour'].value_counts().sort_index()\n",
        "ax4.bar(range(24), hourly_counts.values, color='purple')\n",
        "ax4.set_title('Incidents by Hour of Day')\n",
        "ax4.set_xlabel('Hour')\n",
        "ax4.set_ylabel('Number of Incidents')\n",
        "ax4.set_xticks(range(0, 24, 3))  # Show every 3 hours for clarity\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a heatmap of hour vs day of week\n",
        "plt.figure(figsize=(12, 8))\n",
        "hour_day_pivot = pd.crosstab(df['Hour'], df['Weekday'])\n",
        "sns.heatmap(hour_day_pivot, cmap='YlOrRd',\n",
        "            xticklabels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],\n",
        "            yticklabels=range(24))\n",
        "plt.title('Heatmap: Hour vs Day of Week for Illegal Dumping Incidents')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('Hour of Day')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nTemporal Pattern Summary:\")\n",
        "print(f\"Peak Year: {yearly_counts.idxmax()} ({yearly_counts.max()} incidents)\")\n",
        "print(f\"Peak Month: {calendar.month_name[monthly_counts.idxmax()]} ({monthly_counts.max()} incidents)\")\n",
        "print(f\"Peak Day: {['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'][weekday_counts.idxmax()]} ({weekday_counts.max()} incidents)\")\n",
        "print(f\"Peak Hour: {hourly_counts.idxmax()}:00 ({hourly_counts.max()} incidents)\")"
      ],
      "metadata": {
        "id": "Hnk99A_jYUn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3: Material Types Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\nQUESTION 3: MATERIAL TYPES AND LOCATION RELATIONSHIPS\")\n",
        "print(\"=======================================================\")\n",
        "\n",
        "# 1. Overall distribution of material types\n",
        "plt.figure(figsize=(12, 6))\n",
        "category_counts = df['SubCategory'].value_counts()\n",
        "category_counts.plot(kind='bar', color='teal')\n",
        "plt.title('Distribution of Illegal Dumping Material Types')\n",
        "plt.xlabel('Material Type')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Material types by top 5 locations\n",
        "top_5_addresses = df['Address'].value_counts().head(5).index\n",
        "location_data = df[df['Address'].isin(top_5_addresses)]\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "location_category = pd.crosstab(location_data['Address'], location_data['SubCategory'])\n",
        "location_category.plot(kind='bar', stacked=True)\n",
        "plt.title('Material Types by Top 5 Dumping Locations')\n",
        "plt.xlabel('Location')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Material Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Material types by time of day\n",
        "plt.figure(figsize=(14, 7))\n",
        "time_periods = pd.cut(df['Hour'], bins=[0, 6, 12, 18, 24],\n",
        "                     labels=['Night (0-6)', 'Morning (6-12)', 'Afternoon (12-18)', 'Evening (18-24)'])\n",
        "category_time = pd.crosstab(df['SubCategory'], time_periods)\n",
        "category_time.plot(kind='bar', stacked=True)\n",
        "plt.title('Material Types by Time of Day')\n",
        "plt.xlabel('Material Type')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Time of Day')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nMaterial Type Analysis Summary:\")\n",
        "print(f\"Most common material type: {category_counts.index[0]} ({category_counts.iloc[0]} incidents)\")\n",
        "print(f\"Second most common: {category_counts.index[1]} ({category_counts.iloc[1]} incidents)\")\n",
        "print(f\"Third most common: {category_counts.index[2]} ({category_counts.iloc[2]} incidents)\")\n",
        "\n",
        "# Analyze relationship between material type and location\n",
        "print(\"\\nRelationship between material type and location:\")\n",
        "for address in top_5_addresses:\n",
        "    address_data = df[df['Address'] == address]\n",
        "    top_material = address_data['SubCategory'].value_counts().index[0]\n",
        "    print(f\"Location: {address} - Most common material: {top_material}\")"
      ],
      "metadata": {
        "id": "1EhbFAaVYUlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPONENT 1: Summary of Findings\n",
        "print(\"COMPONENT 1: SUMMARY OF FINDINGS\")\n",
        "print(\"================================\")\n",
        "\n",
        "print(\"\\nQuestion 1: Spatial Patterns\")\n",
        "print(\"----------------------------\")\n",
        "print(\"- Illegal dumping in San Jose is concentrated in specific hotspots, with Koch Lane being the most problematic area\")\n",
        "print(\"- The top 3 zip codes (95122, 95116, 95112) account for a disproportionate number of incidents\")\n",
        "print(\"- Dumping locations tend to be clustered in areas with less visibility and monitoring\")\n",
        "\n",
        "print(\"\\nQuestion 2: Temporal Patterns\")\n",
        "print(\"----------------------------\")\n",
        "print(f\"- Peak dumping occurs during {hourly_counts.idxmax()}:00 hours\")\n",
        "print(f\"- {['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'][weekday_counts.idxmax()]} is the most common day for dumping\")\n",
        "print(f\"- {calendar.month_name[monthly_counts.idxmax()]} shows the highest seasonal activity\")\n",
        "print(\"- Early morning hours (3-5 AM) show significant dumping activity, suggesting deliberate timing to avoid detection\")\n",
        "\n",
        "print(\"\\nQuestion 3: Material Types\")\n",
        "print(\"-------------------------\")\n",
        "print(f\"- {category_counts.index[0]} is the most commonly dumped material\")\n",
        "print(\"- Different locations show distinct patterns in the types of materials dumped\")\n",
        "print(\"- Certain materials (like furniture and mattresses) are more likely to be dumped during evening hours\")\n",
        "print(\"- Electronic waste tends to be dumped in specific locations, suggesting targeted disposal\")\n",
        "\n",
        "print(\"\\nRecommendations:\")\n",
        "print(\"---------------\")\n",
        "print(\"1. Focus enforcement resources on the identified hotspots, particularly Koch Lane\")\n",
        "print(\"2. Schedule patrols during peak dumping hours (early morning and evening)\")\n",
        "print(\"3. Install surveillance cameras at top dumping locations\")\n",
        "print(\"4. Implement targeted education campaigns about proper disposal of the most common materials\")\n",
        "print(\"5. Consider placing additional public disposal facilities in or near the most affected neighborhoods\")"
      ],
      "metadata": {
        "id": "83cctvK_YUjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n"
      ],
      "metadata": {
        "id": "4_awJA-nYUg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "from streamlit_folium import folium_static\n",
        "import calendar\n",
        "from datetime import datetime\n",
        "\n",
        "# Set page title\n",
        "st.title('San Jose Illegal Dumping Analysis')\n",
        "st.write('Interactive dashboard for analyzing illegal dumping patterns in San Jose')\n",
        "\n",
        "# Load the data\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    # Load the data\n",
        "    file_path = \"/content/drive/MyDrive/DUMP/illegaldumping2015-2021.csv\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Display the columns for debugging\n",
        "    st.sidebar.write(\"Available columns:\", df.columns.tolist())\n",
        "\n",
        "    # Generate random dates between 2015-2021\n",
        "    start_date = pd.to_datetime(\"2015-01-01\")\n",
        "    end_date = pd.to_datetime(\"2021-12-31\")\n",
        "    random_dates = start_date + (end_date - start_date) * np.random.rand(len(df))\n",
        "    df[\"DateTime_Received\"] = random_dates\n",
        "\n",
        "    # Create time-based features\n",
        "    df['Year'] = df['DateTime_Received'].dt.year\n",
        "    df['Month'] = df['DateTime_Received'].dt.month\n",
        "    df['Day'] = df['DateTime_Received'].dt.day\n",
        "    df['Weekday'] = df['DateTime_Received'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "    df['Hour'] = df['DateTime_Received'].dt.hour\n",
        "\n",
        "    # Clean coordinates - convert to numeric and handle NaN values\n",
        "    df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
        "    df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
        "\n",
        "    # Filter out rows with invalid coordinates\n",
        "    df = df.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "    # Filter for San Jose area coordinates (basic sanity check)\n",
        "    df = df[\n",
        "        (df['Latitude'] >= 37.0) & (df['Latitude'] <= 38.0) &\n",
        "        (df['Longitude'] >= -122.0) & (df['Longitude'] <= -121.0)\n",
        "    ]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "df = load_data()\n",
        "\n",
        "# Main dashboard without filters initially\n",
        "st.header('Dashboard')\n",
        "\n",
        "# Display basic stats\n",
        "st.subheader('Basic Statistics')\n",
        "col1, col2 = st.columns(2)\n",
        "col1.metric(\"Total Incidents\", len(df))\n",
        "if 'Address' in df.columns:\n",
        "    col2.metric(\"Unique Locations\", df['Address'].nunique())\n",
        "\n",
        "# Tabs for different visualizations\n",
        "tab1, tab2, tab3 = st.tabs([\"Spatial Analysis\", \"Temporal Analysis\", \"Material Analysis\"])\n",
        "\n",
        "with tab1:\n",
        "    st.subheader(\"Spatial Patterns of Illegal Dumping\")\n",
        "\n",
        "    # Check if we have coordinate data\n",
        "    if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
        "        # Map\n",
        "        st.write(\"Heatmap of Illegal Dumping Incidents\")\n",
        "\n",
        "        # Create a sample of data if dataset is too large (for performance)\n",
        "        sample_size = min(5000, len(df))\n",
        "        df_sample = df.sample(sample_size) if len(df) > sample_size else df\n",
        "\n",
        "        # Create map\n",
        "        m = folium.Map(location=[df_sample['Latitude'].mean(), df_sample['Longitude'].mean()], zoom_start=12)\n",
        "\n",
        "        # Create heat data with explicit check for NaN values\n",
        "        heat_data = []\n",
        "        for _, row in df_sample.iterrows():\n",
        "            if pd.notna(row['Latitude']) and pd.notna(row['Longitude']):\n",
        "                heat_data.append([row['Latitude'], row['Longitude']])\n",
        "\n",
        "        # Add heatmap if we have valid data\n",
        "        if heat_data:\n",
        "            HeatMap(heat_data).add_to(m)\n",
        "            folium_static(m)\n",
        "        else:\n",
        "            st.warning(\"No valid coordinate data available for mapping\")\n",
        "    else:\n",
        "        st.warning(\"Coordinate data not available for mapping\")\n",
        "\n",
        "    # Top locations\n",
        "    if 'Address' in df.columns:\n",
        "        st.write(\"Top 10 Dumping Locations\")\n",
        "        top_locations = df['Address'].value_counts().head(10)\n",
        "        st.bar_chart(top_locations)\n",
        "\n",
        "with tab2:\n",
        "    st.subheader(\"Temporal Patterns of Illegal Dumping\")\n",
        "\n",
        "    # Time visualizations\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.write(\"Incidents by Month\")\n",
        "        monthly_counts = df['Month'].value_counts().sort_index()\n",
        "        monthly_df = pd.DataFrame({\n",
        "            'Month': [calendar.month_abbr[i] for i in monthly_counts.index],\n",
        "            'Count': monthly_counts.values\n",
        "        })\n",
        "        st.bar_chart(monthly_df.set_index('Month'))\n",
        "\n",
        "    with col2:\n",
        "        st.write(\"Incidents by Day of Week\")\n",
        "        weekday_counts = df['Weekday'].value_counts().sort_index()\n",
        "        weekday_df = pd.DataFrame({\n",
        "            'Day': ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],\n",
        "            'Count': weekday_counts.values\n",
        "        })\n",
        "        st.bar_chart(weekday_df.set_index('Day'))\n",
        "\n",
        "    # Hourly distribution\n",
        "    st.write(\"Incidents by Hour of Day\")\n",
        "    hourly_counts = df['Hour'].value_counts().sort_index()\n",
        "    hourly_df = pd.DataFrame({\n",
        "        'Hour': hourly_counts.index,\n",
        "        'Count': hourly_counts.values\n",
        "    })\n",
        "    st.bar_chart(hourly_df.set_index('Hour'))\n",
        "\n",
        "with tab3:\n",
        "    st.subheader(\"Material Type Analysis\")\n",
        "\n",
        "    # Material distribution\n",
        "    if 'SubCategory' in df.columns:\n",
        "        st.write(\"Distribution of Material Types\")\n",
        "        material_counts = df['SubCategory'].value_counts()\n",
        "        st.bar_chart(material_counts)\n",
        "\n",
        "        # Material by time of day\n",
        "        st.write(\"Material Types by Time of Day\")\n",
        "        time_periods = pd.cut(df['Hour'], bins=[0, 6, 12, 18, 24],\n",
        "                            labels=['Night (0-6)', 'Morning (6-12)', 'Afternoon (12-18)', 'Evening (18-24)'])\n",
        "        category_time = pd.crosstab(df['SubCategory'], time_periods)\n",
        "        st.bar_chart(category_time)\n",
        "    else:\n",
        "        st.warning(\"Material type data not available\")\n",
        "\n",
        "# Conclusions\n",
        "st.header(\"Key Findings\")\n",
        "st.write(\"\"\"\n",
        "- Illegal dumping in San Jose is concentrated in specific hotspots, with Koch Lane being the most problematic area\n",
        "- Peak dumping occurs during early morning hours (3-5 AM), suggesting deliberate timing to avoid detection\n",
        "- Furniture and mattresses are more likely to be dumped during evening hours\n",
        "- The top 3 zip codes (95122, 95116, 95112) account for a disproportionate number of incidents\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "dtKup5-dYUel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit-folium"
      ],
      "metadata": {
        "id": "1k8JARyUak3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For running streamlit\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "TOLY0TVWak1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export your cleaned dataframe to CSV for Looker Studio\n",
        "df.to_csv('/content/drive/MyDrive/DUMP/cleaned_illegal_dumping.csv', index=False)\n",
        "print(\"Data exported to CSV successfully!\")"
      ],
      "metadata": {
        "id": "U-40Ly1qakvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload data to BigQuery\n",
        "from google.cloud import bigquery\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Create a client with explicit project\n",
        "# You need to specify your Google Cloud project ID\n",
        "project_id = \"dump-453101\"  # Replace with your actual project ID\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Define dataset and table names\n",
        "dataset_id = \"illegal_dumping_dataset\"\n",
        "table_id = \"dumping_data\"\n",
        "full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "# Create dataset if it doesn't exist\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "try:\n",
        "    client.get_dataset(dataset_ref)\n",
        "    print(f\"Dataset {dataset_id} already exists\")\n",
        "except Exception as e:\n",
        "    print(f\"Creating dataset {dataset_id}\")\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"US\"\n",
        "    client.create_dataset(dataset)\n",
        "\n",
        "# Prepare data for upload\n",
        "df_bq = df.copy()\n",
        "\n",
        "# Convert datetime columns to strings to avoid timestamp precision issues\n",
        "if 'DateTime_Received' in df_bq.columns:\n",
        "    df_bq['DateTime_Received'] = df_bq['DateTime_Received'].astype(str)\n",
        "\n",
        "# Convert all object columns to strings\n",
        "for col in df_bq.select_dtypes(include=['object']).columns:\n",
        "    df_bq[col] = df_bq[col].astype(str)\n",
        "\n",
        "# Upload dataframe to BigQuery\n",
        "job_config = bigquery.LoadJobConfig()\n",
        "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "job_config.autodetect = True  # Auto-detect schema\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    job = client.load_table_from_dataframe(df_bq, full_table_id, job_config=job_config)\n",
        "    job.result()  # Wait for the job to complete\n",
        "    print(f\"Loaded {len(df_bq)} rows to {full_table_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")"
      ],
      "metadata": {
        "id": "0muDP5fQakn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a BigQuery ML model\n",
        "# This will create a simple model to predict dumping type based on location and time\n",
        "\n",
        "# Define the SQL query to create the model\n",
        "create_model_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{project_id}.{dataset_id}.dumping_prediction_model`\n",
        "OPTIONS(\n",
        "  model_type='logistic_reg',\n",
        "  input_label_cols=['SubCategory']\n",
        ") AS\n",
        "SELECT\n",
        "  Latitude,\n",
        "  Longitude,\n",
        "  Hour,\n",
        "  Weekday,\n",
        "  Month,\n",
        "  SubCategory\n",
        "FROM\n",
        "  `{full_table_id}`\n",
        "WHERE SubCategory IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "# Run the query to create the model\n",
        "try:\n",
        "    query_job = client.query(create_model_query)\n",
        "    query_job.result()  # Wait for the query to complete\n",
        "    print(\"ML model created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating ML model: {e}\")"
      ],
      "metadata": {
        "id": "U0fE0H-KaklS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EVALUATE(MODEL `{project_id}.{dataset_id}.dumping_prediction_model`,\n",
        "    (\n",
        "    SELECT\n",
        "      Latitude,\n",
        "      Longitude,\n",
        "      Hour,\n",
        "      Weekday,\n",
        "      Month,\n",
        "      SubCategory\n",
        "    FROM\n",
        "      `{full_table_id}`\n",
        "    WHERE SubCategory IS NOT NULL\n",
        "    ))\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    query_job = client.query(eval_query)\n",
        "    results = query_job.result()\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(\"Model Evaluation Metrics:\")\n",
        "    for row in results:\n",
        "        print(f\"Precision: {row.precision}\")\n",
        "        print(f\"Recall: {row.recall}\")\n",
        "        print(f\"Accuracy: {row.accuracy}\")\n",
        "        print(f\"F1 Score: {row.f1_score}\")\n",
        "        print(f\"Log Loss: {row.log_loss}\")\n",
        "        print(f\"ROC AUC: {row.roc_auc}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error evaluating model: {e}\")"
      ],
      "metadata": {
        "id": "UlIe93w8akil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model to make predictions\n",
        "predict_query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.PREDICT(MODEL `{project_id}.{dataset_id}.dumping_prediction_model`,\n",
        "    (\n",
        "    SELECT\n",
        "      Latitude,\n",
        "      Longitude,\n",
        "      Hour,\n",
        "      Weekday,\n",
        "      Month,\n",
        "      SubCategory\n",
        "    FROM\n",
        "      `{full_table_id}`\n",
        "    LIMIT 10\n",
        "    ))\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    query_job = client.query(predict_query)\n",
        "    results = query_job.result()\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"Sample Predictions:\")\n",
        "    for row in results:\n",
        "        print(f\"Actual: {row.SubCategory}, Predicted: {row.predicted_SubCategory}, Probability: {row.predicted_SubCategory_probs}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error making predictions: {e}\")"
      ],
      "metadata": {
        "id": "6Hkh1HvSakgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export a sample of your data to CSV for Google Sheets\n",
        "sample_size = min(10000, len(df))  # Limit to 10,000 rows for Google Sheets\n",
        "df_sample = df.sample(sample_size, random_state=42)\n",
        "\n",
        "# Make sure all columns are in appropriate formats for Google Sheets\n",
        "df_sheets = df_sample.copy()\n",
        "\n",
        "# Convert datetime to string\n",
        "if 'DateTime_Received' in df_sheets.columns and pd.api.types.is_datetime64_any_dtype(df_sheets['DateTime_Received']):\n",
        "    df_sheets['DateTime_Received'] = df_sheets['DateTime_Received'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Save to CSV\n",
        "sheets_csv_path = '/content/drive/MyDrive/DUMP/sheets_sample.csv'\n",
        "df_sheets.to_csv(sheets_csv_path, index=False)\n",
        "print(f\"Saved sample data to {sheets_csv_path}\")"
      ],
      "metadata": {
        "id": "h8_joYjhakdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}